{% extends "_base.html" %}

{% block title %}Bots information - Repology{% endblock %}
{% block header %}Repology bots{% endblock %}

{% block content %}
<div class="container">

<p>You're likely seeing this because your site had been visited by Repology.</p>

<h3>What's repology?</h3>

<p>Repology is a free and open source service which monitors a huge number of
<a href="https://en.wikipedia.org/wiki/Package_(package_management_system)">package</a>
<a href="https://en.wikipedia.org/wiki/Software_repository">repositories</a>,
comparing versions of packaged software across them and gathering other information
on free and open source projects which may be useful to the
<a href="https://en.wikipedia.org/wiki/Free_and_Open-Source_Software">F/OSS</a>
community.</p>

<h3>What robots are used by repology?</h3>

<h4>repology-fetcher</h4>

<p>Identifies itself as <code>repology-fetcher/0 (+{{ url_for('docs_bots', _external=True) }})</code></p>

<p>This process regularly retrieves information from software repositories. The
preferred way is to get a single file which describes all the available packages,
but for repositories which don't support this the robot may iterate over some
web API. The robot visits a site on each update cycle (~2-3 hours currently) and
fetches files it needs sequentially (e.g. it never does parallel requests).</p>

<p>You may find metadata on which repositories are fetched
<a href="{{ config['REPOLOGY_UPDATER_REPO_URL'] }}/tree/master/repos.d">here</a>
and the fetcher code
<a href="{{ config['REPOLOGY_UPDATER_REPO_URL'] }}/tree/master/repology/fetchers">here</a>.
</p>

<p>If you think the robot creates excess load on your site, feel free to drop an
<a href="{{ config['REPOLOGY_UPDATER_REPO_URL'] }}/issues/new">issue</a> in the GitHub.
If Repology gets information on your repository through web API, we'd greatly
appreciate if you provide a regular dump of package information from your repository
(data used by Repology include package name, version, one-line summary, list of maintainers,
list of categories/tags, homepage and download URLs, license information) in machine readable
format (preferably JSON) as well. This will allow more frequent updates with less load
on the repository side, and faster update process, simpler parsing code and probably more
useful data for Repology.</p>

<h4>repology-linkchecker</h4>

<p>Identifies itself as <code>repology-linkchecker/1 (+{{ url_for('docs_bots', _external=True) }})</code></p>

<p>This process pokes links retrieved from package metadata to check that they
are alive. Dead links and links which involve redirects are reported to package
maintainers so the package metadata could be correspondingly updated. If this robot
visits you site, this means it is mentioned in some package metadata.</p>

<p>The process visits each link once a week. It issues
<a href="https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol#Request_methods">HEAD</a>
request first, and only if that fails it falls back to a
<a href="https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol#Request_methods">GET</a>
request. This means that in most cases the robot won't retrieve the contents of a URL,
using only marginal amount of web traffic. Also, there's a delay of 3 seconds between
consecutive requests to a single hostname, to ensure no excess site load is generated.</p>

<p>You may see the link checker source code
<a href="{{ config['REPOLOGY_LINKCHECKER_REPO_URL'] }}">here</a>.</p>

<h4>repology-vulnupdater</h4>

<p>Identifies itself as <code>repology-vulnupdater/1 (+{{ url_for('docs_bots', _external=True) }})</code></p>

<p>This process maintains up to date information on software security vulnerabilities
in Repology by periodically fetching <a href="https://nvd.nist.gov/vuln/data-feeds#JSON_FEED">NVD JSON feeds</a>.
It issues a <a href="https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol#Request_methods">GET</a>
request to each feed every 10 minutes, <code>Etag</code> and <code>If-None-Match</code> HTTP headers
are used to avoid refetching the files if they have not changed since the last request.</p>

<p>You may see the vulnupdater's source code
<a href="{{ config['REPOLOGY_VULNUPDATER_REPO_URL'] }}">here</a>.</p>

<h3>robots.txt policy</h3>

<p>Please note that none of our robots is a crawler. Unlike most search engines which would
try to gather all available URLs from a specific website, so it may be required to restrict
them through <a href="https://en.wikipedia.org/wiki/Robots_exclusion_standard">robots.txt</a>
file, Repology only interacts with a <strong>fixed</strong> small set of <strong>man-made</strong>
links, and needs unconditional access to them to perform its tasks (e.g. retrieving repository
information and link availability checking), so neither of repology robots respects
<a href="https://en.wikipedia.org/wiki/Robots_exclusion_standard">robots.txt</a>.</p>

</div> {#- container #}
{% endblock %}
